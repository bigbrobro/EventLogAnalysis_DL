{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have TensorFlow version 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import utils, preprocessing\n",
    "\n",
    "# This code was tested with TensorFlow v1.4\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\", dtype = {'eventid':'object','result':'object'})\n",
    "test_data= pd.read_csv(\"test.csv\")\n",
    "token_data = 'dll_token.pickle'\n",
    "encode_data = 'dll_encode.pickle'\n",
    "model_data = 'dll_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventid</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4624 4769 4624 4624 4769 4768 4769 4768 4624 4...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4624 4624 4624 4624 4769 4624 4624 4624 4624 4...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4624 4769 4769 4768 4624 4624 4769 4768 4624 4...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5140 5140 5140 5140 5140 5140 5140 5140 5140 5...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4672 4672 4672 4672 4672 4672 4672 4672 4672 4...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             eventid  result\n",
       "0  4624 4769 4624 4624 4769 4768 4769 4768 4624 4...  normal\n",
       "1  4624 4624 4624 4624 4769 4624 4624 4624 4624 4...  normal\n",
       "2  4624 4769 4769 4768 4624 4624 4769 4768 4624 4...  normal\n",
       "3  5140 5140 5140 5140 5140 5140 5140 5140 5140 5...  normal\n",
       "4  4672 4672 4672 4672 4672 4672 4672 4672 4672 4...  normal"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Part of training data\")\n",
    "data['eventid'].value_counts()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal    250\n",
       "attack     57\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_num = data['result'].nunique()\n",
    "data['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words  = 10000\n",
    "tokenizer = text.Tokenizer(num_words=max_words, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "max_len = 50\n",
    "print(type(data['eventid']))\n",
    "tokenizer.fit_on_texts(data['eventid'])\n",
    "sequences = tokenizer.texts_to_sequences(data['eventid'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['eventid'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data_sec = pad_sequences(sequences, maxlen=max_len)\n",
    "test_data_sec = pad_sequences(test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the token data in a file\n",
    "with open(token_data, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train data: 307\n",
      "The number of test data: 0\n"
     ]
    }
   ],
   "source": [
    "# Split data into train data and test data\n",
    "train_size = int(len(data_sec) * 1.0)\n",
    "print (\"The number of train data: %d\" % train_size)\n",
    "print (\"The number of test data: %d\" % (len(data_sec) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_sec[:train_size]\n",
    "x_test = test_data_sec\n",
    "\n",
    "test_posts_doc = test_data['eventid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = data['result'][:train_size]\n",
    "test_tags = test_data['result']\n",
    "\n",
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the encoder in a file\n",
    "with open(encode_data, 'wb') as handle:\n",
    "    pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307, 2)\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 50, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,300,674\n",
      "Trainable params: 1,300,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128, input_length=max_len))\n",
    "#model.add(Flatten())\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(tag_num, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #optimizer='adam',\n",
    "              optimizer='rmsprop',\n",
    "              #optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model trains very quickly and 2 epochs are already more than enough\n",
    "# Training for more epochs will likely lead to overfitting on this dataset\n",
    "# You can try tweaking these hyperparamaters when using this model with your own data\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (307, 50)\n",
      "x_test shape: (57, 50)\n",
      "y_train shape: (307, 2)\n",
      "y_test shape: (57, 2)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 307 samples, validate on 57 samples\n",
      "Epoch 1/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3561 - acc: 0.8681 - val_loss: 1.1969 - val_acc: 0.2895\n",
      "Epoch 2/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3557 - acc: 0.8583 - val_loss: 1.2398 - val_acc: 0.2281\n",
      "Epoch 3/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3498 - acc: 0.8599 - val_loss: 1.2113 - val_acc: 0.3246\n",
      "Epoch 4/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3436 - acc: 0.8664 - val_loss: 1.2385 - val_acc: 0.3772\n",
      "Epoch 5/50\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3519 - acc: 0.8681 - val_loss: 1.1210 - val_acc: 0.4123\n",
      "Epoch 6/50\n",
      "307/307 [==============================] - 1s 3ms/step - loss: 0.3403 - acc: 0.8762 - val_loss: 1.1875 - val_acc: 0.3684\n",
      "Epoch 7/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3408 - acc: 0.8713 - val_loss: 1.1855 - val_acc: 0.4123\n",
      "Epoch 8/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3342 - acc: 0.8762 - val_loss: 1.0628 - val_acc: 0.4298\n",
      "Epoch 9/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3309 - acc: 0.8811 - val_loss: 1.0368 - val_acc: 0.4561\n",
      "Epoch 10/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3325 - acc: 0.8746 - val_loss: 1.1067 - val_acc: 0.4386\n",
      "Epoch 11/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3256 - acc: 0.8746 - val_loss: 1.1446 - val_acc: 0.4386\n",
      "Epoch 12/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3190 - acc: 0.8795 - val_loss: 1.1334 - val_acc: 0.4298\n",
      "Epoch 13/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3398 - acc: 0.8567 - val_loss: 1.0159 - val_acc: 0.4912\n",
      "Epoch 14/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3174 - acc: 0.8811 - val_loss: 0.9785 - val_acc: 0.5526\n",
      "Epoch 15/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3108 - acc: 0.8827 - val_loss: 1.0987 - val_acc: 0.4737\n",
      "Epoch 16/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3126 - acc: 0.8811 - val_loss: 0.9695 - val_acc: 0.5175\n",
      "Epoch 17/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3641 - acc: 0.8143 - val_loss: 1.0916 - val_acc: 0.4298\n",
      "Epoch 18/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3566 - acc: 0.8860 - val_loss: 1.0306 - val_acc: 0.4386\n",
      "Epoch 19/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3363 - acc: 0.8844 - val_loss: 1.0684 - val_acc: 0.4035\n",
      "Epoch 20/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3351 - acc: 0.8795 - val_loss: 1.0254 - val_acc: 0.4211\n",
      "Epoch 21/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3312 - acc: 0.8779 - val_loss: 1.0211 - val_acc: 0.4649\n",
      "Epoch 22/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3307 - acc: 0.8811 - val_loss: 1.0751 - val_acc: 0.3947\n",
      "Epoch 23/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3286 - acc: 0.8795 - val_loss: 1.0610 - val_acc: 0.4035\n",
      "Epoch 24/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3218 - acc: 0.8811 - val_loss: 1.0705 - val_acc: 0.4474\n",
      "Epoch 25/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3193 - acc: 0.8827 - val_loss: 1.0622 - val_acc: 0.4211\n",
      "Epoch 26/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3210 - acc: 0.8795 - val_loss: 1.0443 - val_acc: 0.4386\n",
      "Epoch 27/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3215 - acc: 0.8844 - val_loss: 1.0743 - val_acc: 0.4123\n",
      "Epoch 28/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3206 - acc: 0.8795 - val_loss: 1.0233 - val_acc: 0.4649\n",
      "Epoch 29/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3084 - acc: 0.8795 - val_loss: 1.0654 - val_acc: 0.4298\n",
      "Epoch 30/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3068 - acc: 0.8860 - val_loss: 1.0762 - val_acc: 0.4123\n",
      "Epoch 31/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3090 - acc: 0.8909 - val_loss: 1.0588 - val_acc: 0.4298\n",
      "Epoch 32/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3077 - acc: 0.8762 - val_loss: 1.0921 - val_acc: 0.4035\n",
      "Epoch 33/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3091 - acc: 0.8860 - val_loss: 0.9682 - val_acc: 0.4737\n",
      "Epoch 34/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2998 - acc: 0.8876 - val_loss: 1.1320 - val_acc: 0.4123\n",
      "Epoch 35/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3031 - acc: 0.8827 - val_loss: 1.0045 - val_acc: 0.4298\n",
      "Epoch 36/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3013 - acc: 0.8860 - val_loss: 1.0981 - val_acc: 0.4825\n",
      "Epoch 37/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3214 - acc: 0.8795 - val_loss: 1.0217 - val_acc: 0.4825\n",
      "Epoch 38/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3042 - acc: 0.8779 - val_loss: 1.0068 - val_acc: 0.4737\n",
      "Epoch 39/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3037 - acc: 0.8762 - val_loss: 0.9788 - val_acc: 0.4561\n",
      "Epoch 40/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3028 - acc: 0.8844 - val_loss: 1.0068 - val_acc: 0.4737\n",
      "Epoch 41/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2943 - acc: 0.8827 - val_loss: 0.9893 - val_acc: 0.4561\n",
      "Epoch 42/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3044 - acc: 0.8860 - val_loss: 1.0807 - val_acc: 0.4737\n",
      "Epoch 43/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.3059 - acc: 0.8844 - val_loss: 1.0486 - val_acc: 0.4649\n",
      "Epoch 44/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2996 - acc: 0.8844 - val_loss: 0.9856 - val_acc: 0.4649\n",
      "Epoch 45/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2922 - acc: 0.8876 - val_loss: 0.9462 - val_acc: 0.4825\n",
      "Epoch 46/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2928 - acc: 0.8762 - val_loss: 0.9912 - val_acc: 0.4649\n",
      "Epoch 47/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2920 - acc: 0.8795 - val_loss: 0.9617 - val_acc: 0.4649\n",
      "Epoch 48/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2888 - acc: 0.8844 - val_loss: 1.0308 - val_acc: 0.4123\n",
      "Epoch 49/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2998 - acc: 0.8811 - val_loss: 0.9284 - val_acc: 0.4649\n",
      "Epoch 50/50\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 0.2873 - acc: 0.8746 - val_loss: 1.0036 - val_acc: 0.4298\n"
     ]
    }
   ],
   "source": [
    "# model.fit trains the model# model. \n",
    "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
    "# You can see the validation loss decreasing slowly when you run this\n",
    "# Because val_loss is no longer decreasing we stop training to prevent overfitting\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 449us/step\n",
      "Test score: 1.0036012195704276\n",
      "Test accuracy: 0.42982455669787895\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model if you want to\n",
    "model.save(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "\n",
    "text_labels = encoder.classes_ \n",
    "\n",
    "import csv\n",
    "with open('result.csv', 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    writer.writerow(['Actual label','Predicted label','dlls']) \n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        #print(np.array([x_test[i]]))\n",
    "        prediction = model.predict(np.array([x_test[i]]))\n",
    "        predicted_label = text_labels[np.argmax(prediction)]\n",
    "        act_label=str(test_tags.iloc[i])\n",
    "        dlls=str(test_posts_doc.iloc[i])\n",
    "        writer.writerow([act_label,predicted_label,dlls]) \n",
    "        #print(test_posts_doc.iloc[i])\n",
    "        #print(prediction)\n",
    "        #print('Actual label:' + test_tags.iloc[i])\n",
    "        #print('Predicted label: ' + predicted_label + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden attack\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 5 1 5 5]]\n",
      "[[0.09676401 0.9078897 ]]\n",
      "normal\n",
      "Eternal attack\n",
      "[[0.14978842 0.84790593]]\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "#golden\n",
    "print('golden attack')\n",
    "request = '''\n",
    "4624 4769 4624 4769 4769 \n",
    "'''\n",
    "request = [request]\n",
    "\n",
    "req_mat = tokenizer.texts_to_sequences(request)\n",
    "data_sec = pad_sequences(req_mat, maxlen=max_len)\n",
    "prediction = model.predict(np.array(data_sec))\n",
    "print(np.array(data_sec))\n",
    "predicted_label = encoder.classes_[np.argmax(prediction)]\n",
    "print(prediction)\n",
    "print(predicted_label)\n",
    "\n",
    "print('Eternal attack')\n",
    "request = '''\n",
    "5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 5140 \n",
    "'''\n",
    "request = [request]\n",
    "\n",
    "req_mat = tokenizer.texts_to_sequences(request)\n",
    "data_sec = pad_sequences(req_mat, maxlen=max_len)\n",
    "prediction = model.predict(np.array(data_sec))\n",
    "predicted_label = encoder.classes_[np.argmax(prediction)]\n",
    "print(prediction)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4624': 1, '4672': 2, '5140': 3, '4674': 4, '4769': 5, '4768': 6, '4688': 7, '4673': 8, '4776': 9}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
